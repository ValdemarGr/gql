"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[137],{3905:(e,n,t)=>{t.d(n,{Zo:()=>p,kt:()=>u});var a=t(7294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=a.createContext({}),c=function(e){var n=a.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},p=function(e){var n=c(e.components);return a.createElement(l.Provider,{value:n},e.children)},m={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},d=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),d=c(t),u=r,h=d["".concat(l,".").concat(u)]||d[u]||m[u]||o;return t?a.createElement(h,i(i({ref:n},p),{},{components:t})):a.createElement(h,i({ref:n},p))}));function u(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var o=t.length,i=new Array(o);i[0]=d;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var c=2;c<o;c++)i[c]=t[c];return a.createElement.apply(null,i)}return a.createElement.apply(null,t)}d.displayName="MDXCreateElement"},1631:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var a=t(7462),r=(t(7294),t(3905));const o={title:"Resolvers"},i=void 0,s={unversionedId:"schema/resolvers",id:"schema/resolvers",title:"Resolvers",description:"Resolvers are where the most interest should lie, since they act as the layer between input type and next continuation.",source:"@site/docs/schema/resolvers.md",sourceDirName:"schema",slug:"/schema/resolvers",permalink:"/gql/docs/schema/resolvers",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/schema/resolvers.md",tags:[],version:"current",frontMatter:{title:"Resolvers"},sidebar:"docs",previous:{title:"The DSL",permalink:"/gql/docs/schema/dsl"},next:{title:"The schema",permalink:"/gql/docs/schema/"}},l={},c=[{value:"EffectResolver",id:"effectresolver",level:2},{value:"BatchResolver",id:"batchresolver",level:2},{value:"Example of a database batcher",id:"example-of-a-database-batcher",level:3},{value:"Design patterns",id:"design-patterns",level:3},{value:"StreamResolver",id:"streamresolver",level:2},{value:"Stream semantics",id:"stream-semantics",level:3},{value:"Interesting use cases",id:"interesting-use-cases",level:3}],p={toc:c};function m(e){let{components:n,...t}=e;return(0,r.kt)("wrapper",(0,a.Z)({},p,t,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"Resolvers are where the most interest should lie, since they act as the layer between input type and next continuation.\nThe raw resolver types are as expressive as possible to allow as many use cases as possible, which can cause a lot of noise in the daily use of gql.\nTherefore the ",(0,r.kt)("inlineCode",{parentName:"p"},"dsl")," should be enough to get started and this section should act as an introduction for the curious."),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"The error types have been omitted from the resolver types for brevity.")),(0,r.kt)("h2",{id:"effectresolver"},"EffectResolver"),(0,r.kt)("p",null,"The simplest resolver is the effect resolver ",(0,r.kt)("inlineCode",{parentName:"p"},"EffectResolver[F, I, A]")," which takes ",(0,r.kt)("inlineCode",{parentName:"p"},"I")," to ",(0,r.kt)("inlineCode",{parentName:"p"},"F[A]"),"."),(0,r.kt)("h2",{id:"batchresolver"},"BatchResolver"),(0,r.kt)("p",null,"The batch resolver ",(0,r.kt)("inlineCode",{parentName:"p"},"BatchResolver[F, I, A]")," allows the interpreter to more effeciently fetch data.\nThe resolver captures a the following steps:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"It takes ",(0,r.kt)("inlineCode",{parentName:"li"},"I")," to ",(0,r.kt)("inlineCode",{parentName:"li"},"F[Set[K]]")," for some ",(0,r.kt)("inlineCode",{parentName:"li"},"K")),(0,r.kt)("li",{parentName:"ul"},"Then merges the keys ",(0,r.kt)("inlineCode",{parentName:"li"},"Set[K]")," from many different resolvers into a single ",(0,r.kt)("inlineCode",{parentName:"li"},"Set[K]")),(0,r.kt)("li",{parentName:"ul"},"Then it fetches the values using a user-defined function ",(0,r.kt)("inlineCode",{parentName:"li"},"Set[K] => F[Map[K, T]]")," for some ",(0,r.kt)("inlineCode",{parentName:"li"},"T")),(0,r.kt)("li",{parentName:"ul"},"Finally it maps the values ",(0,r.kt)("inlineCode",{parentName:"li"},"Map[K, T]")," to ",(0,r.kt)("inlineCode",{parentName:"li"},"F[A]"))),(0,r.kt)("p",null,"The types ",(0,r.kt)("inlineCode",{parentName:"p"},"K")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"T")," are existentially quantified; they are not visible to the user.\nThe base-case implementation for ",(0,r.kt)("inlineCode",{parentName:"p"},"BatchResolver")," has ",(0,r.kt)("inlineCode",{parentName:"p"},"K = Set[I]")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"T = Map[I, A]"),"."),(0,r.kt)("admonition",{type:"info"},(0,r.kt)("p",{parentName:"admonition"},"The resolver will automatically construct a GraphQL error if any of the keys are missing.\nTo avoid this, you must pad all missing keys.\nFor instance, you could map all values to ",(0,r.kt)("inlineCode",{parentName:"p"},"Some")," and pad all missing values with ",(0,r.kt)("inlineCode",{parentName:"p"},"None"),".")),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"BatchResolver")," must also have an implementation of ",(0,r.kt)("inlineCode",{parentName:"p"},"Set[K] => F[Map[K, T]]"),", which is constructed globally."),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"The ",(0,r.kt)("inlineCode",{parentName:"p"},"BatchResolver")," cannot directly embed ",(0,r.kt)("inlineCode",{parentName:"p"},"Set[K] => F[Map[K, T]]"),", since this would allow ambiguity.\nWhat if two ",(0,r.kt)("inlineCode",{parentName:"p"},"BatchResolver"),"'s were to have their keys merged, what resolver's ",(0,r.kt)("inlineCode",{parentName:"p"},"Set[K] => F[Map[K, T]]")," should be used?")),(0,r.kt)("p",null,"A ",(0,r.kt)("inlineCode",{parentName:"p"},"BatchResolver[F, K, T]")," is constructed as follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},"import gql.resolver._\nimport cats.effect._\n\nval brState = BatchResolver[IO, Int, Int](keys => IO.pure(keys.map(k => k -> (k * 2)).toMap))\n// brState: cats.data.package.State[gql.SchemaState[IO], BatchResolver[IO, Set[Int], Map[Int, Int]]] = cats.data.IndexedStateT@1b6eee33\n")),(0,r.kt)("p",null,"A ",(0,r.kt)("inlineCode",{parentName:"p"},"State")," monad is used to keep track of the batchers that have been created and unique id generation.\nDuring schema construction, ",(0,r.kt)("inlineCode",{parentName:"p"},"State")," can be composed using ",(0,r.kt)("inlineCode",{parentName:"p"},"Monad"),"ic operations.\nThe ",(0,r.kt)("inlineCode",{parentName:"p"},"Schema")," companion object contains smart constructors that run the ",(0,r.kt)("inlineCode",{parentName:"p"},"State")," monad."),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"map")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"contramap")," can be used to align the input and output types:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import gql._\nimport gql.dsl._\nimport cats._\nimport cats.implicits._\n\ndef batchSchema = brState.map { (br: BatchResolver[IO, Set[Int], Map[Int, Int]]) =>\n  val adjusted: BatchResolver[IO, Int, Option[Int]] = br\n    .contramap[Int](Set(_))\n    .map { case (_, m) => m.values.headOption }\n\n  SchemaShape[IO, Unit, Unit, Unit](\n    tpe[IO, Unit](\n      "Query",\n      "field" -> field(adjusted.contramap(_ => 42))\n    ).some\n  )\n}\n')),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"There are more formulations of batch resolvers that can be possible, but the chosen one has the least overhead for the developer."),(0,r.kt)("p",{parentName:"admonition"},"One could let the developer declare batch resolvers in-line and explicitly name them.\nThis would impose the validation constraint that all batch resolvers with the same name must have the same function address, or else there would be ambiguity.\nReasoning with function addreses is not very intuitive, so this is not the preferred formulation.")),(0,r.kt)("p",null,"Which we can finally run:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import cats.effect.unsafe.implicits.global\nimport cats.implicits._\n\ndef query = """\n  query {\n    field\n  }\n"""\n\nSchema.stateful(batchSchema).flatMap{ sch =>\n  sch.assemble(query, variables = Map.empty)\n    .traverse { case Executable.Query(run) => run(()).map(_.asGraphQL) }\n}.unsafeRunSync()\n// res0: Either[parser.package.ParseError, io.circe.JsonObject] = Right(\n//   value = object[data -> {\n//   "field" : 84\n// }]\n// )\n')),(0,r.kt)("admonition",{type:"tip"},(0,r.kt)("p",{parentName:"admonition"},"The ",(0,r.kt)("inlineCode",{parentName:"p"},"BatchResolver")," de-duplicates keys since it uses ",(0,r.kt)("inlineCode",{parentName:"p"},"Set")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"Map"),".\nThis means that even if no function exists that effeciently fetches your data, you can still use the ",(0,r.kt)("inlineCode",{parentName:"p"},"BatchResolver")," to de-duplicates it.")),(0,r.kt)("admonition",{type:"tip"},(0,r.kt)("p",{parentName:"admonition"},"The ",(0,r.kt)("inlineCode",{parentName:"p"},"BatchResolver")," does not maintain ordering internally, but this doesn't mean that the output values cannot maintain order."),(0,r.kt)("pre",{parentName:"admonition"},(0,r.kt)("code",{parentName:"pre",className:"language-scala"},"def br: BatchResolver[IO, Set[Int], Map[Int, String]] = ???\n\ndef orderedBr: BatchResolver[IO, List[Int], List[String]] =\n  br.contramap[List[Int]](_.toSet).map{ case (i, m) => i.map(m.apply) }\n"))),(0,r.kt)("admonition",{type:"tip"},(0,r.kt)("p",{parentName:"admonition"},"For more information on how the batch resolver works, check out the ",(0,r.kt)("a",{parentName:"p",href:"/gql/docs/execution/planning"},"planning section"),".")),(0,r.kt)("h3",{id:"example-of-a-database-batcher"},"Example of a database batcher"),(0,r.kt)("p",null,"Most applications interact with a database one way or another.\nUsually databases have a way to fetch multiple rows at once, and it is usually more efficient to do so."),(0,r.kt)("p",null,"Let's define our database:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'trait DatabaseConnection[F[_]] {\n  def get(ids: Set[Int]): F[Map[Int, String]]\n}\n\nobject DatabaseConnection {\n  def apply[F[_]](implicit F: Applicative[F]) = new DatabaseConnection[F] {\n    def get(ids: Set[Int]): F[Map[Int, String]] = {\n      println(s"executing query for ids $ids")\n      F.pure(ids.map(i => i -> s"row $i").toMap)\n    }\n  }\n}\n')),(0,r.kt)("p",null,"Now we can define our schema:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'final case class Nested(key: Int)\n\ndef databaseRoot[F[_]: Monad](implicit db: DatabaseConnection[F]) =\n  BatchResolver[F, Int, String](keys => db.get(keys)).map { br =>\n    val single = br.contramap[Int](Set(_)).map { case (_, m) => m.values.head }\n\n    implicit lazy val nestedType = tpe[F, Nested](\n      "Nested",\n      "nestedValue" -> field(single.contramap[Nested](_.key))\n    )\n    \n    SchemaShape[F, Unit, Unit, Unit](\n      tpe[F, Unit](\n        "Query",\n        "getFirstField" -> field(arg[Int]("x"))(single.contramap{ case (_, x) => x}),\n        "getSecondField" -> field(arg[Int]("y"))(single.contramap{ case (_, x) => x}),\n        "nested" -> pure(_ => Nested(42))\n      ).some\n    )\n  }\n')),(0,r.kt)("p",null,"And finally execute it:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'def databaseQuery = """\n  query {\n    getFirstField(x: 1)\n    getSecondField(y: 2)\n    nested {\n      nestedValue\n    }\n  }\n"""\n\nimplicit def db = DatabaseConnection[IO]\n\nSchema.stateful(databaseRoot[IO]).flatMap{ sch =>\n  sch.assemble(databaseQuery, variables = Map.empty)\n    .traverse { case Executable.Query(run) => run(()).map(_.asGraphQL) }\n}.unsafeRunSync()\n// executing query for ids Set(1, 2, 42)\n// res1: Either[parser.package.ParseError, io.circe.JsonObject] = Right(\n//   value = object[data -> {\n//   "nested" : {\n//     "nestedValue" : "row 42"\n//   },\n//   "getSecondField" : "row 2",\n//   "getFirstField" : "row 1"\n// }]\n// )\n')),(0,r.kt)("p",null,"Notice how the huristic query planner is able to figure out that waiting till ",(0,r.kt)("inlineCode",{parentName:"p"},"nested")," is resolved and then batching is more efficient than batching the two toplevel fields first and then resolving ",(0,r.kt)("inlineCode",{parentName:"p"},"nested"),"."),(0,r.kt)("h3",{id:"design-patterns"},"Design patterns"),(0,r.kt)("p",null,"Since ",(0,r.kt)("inlineCode",{parentName:"p"},"State")," itself is a monad, we can compose them into ",(0,r.kt)("inlineCode",{parentName:"p"},"case class"),"es for more ergonomic implementation at scale."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},"import cats.implicits._\n\ntrait User\ntrait UserId\n\ntrait Company\ntrait CompanyId\n\nfinal case class DomainBatchers[F[_]](\n  userBatcher: BatchResolver[F, Set[UserId], Map[UserId, User]],\n  companyBatcher: BatchResolver[F, Set[CompanyId], Map[CompanyId, Company]],\n  doubleSumBatcher: BatchResolver[F, Int, Int]\n)\n\n(\n  BatchResolver[IO, UserId, User](_ => ???),\n  BatchResolver[IO, CompanyId, Company](_ => ???),\n  BatchResolver[IO, Int, Int](is => IO.pure(is.map(i => i -> (i * 2)).toMap)).map(\n    _\n    .contramap[Int](Set(_))\n    .map[Int]{ case (_, m) => m.values.toList.combineAll }\n  )\n).mapN(DomainBatchers.apply)\n// res2: data.IndexedStateT[Eval, SchemaState[IO], SchemaState[IO], DomainBatchers[[A]IO[A]]] = cats.data.IndexedStateT@d57f54b\n")),(0,r.kt)("h2",{id:"streamresolver"},"StreamResolver"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"StreamResolver")," is a very powerful resolver type, that can perform many different tasks.\nFirst and foremost a ",(0,r.kt)("inlineCode",{parentName:"p"},"StreamResolver")," can update a sub-tree of the schema via some provided stream, like signals in frp or observables."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'def streamSchema = \n  SchemaShape[IO, Unit, Unit, Unit](\n    subscription = tpe[IO, Unit](\n      "Subscription",\n      "stream" -> field(stream(_ => fs2.Stream(1).repeat.lift[IO].scan(0)(_ + _)))\n    ).some\n  )\n')),(0,r.kt)("h3",{id:"stream-semantics"},"Stream semantics"),(0,r.kt)("p",null,"A ",(0,r.kt)("inlineCode",{parentName:"p"},"StreamResolver")," that occurs as a child another ",(0,r.kt)("inlineCode",{parentName:"p"},"StreamResolver")," has some interesting implications."),(0,r.kt)("p",null,"The first time the interpreter sees a ",(0,r.kt)("inlineCode",{parentName:"p"},"StreamResolver"),", it will await the first element and subscribe the rest of the stream to a background queue."),(0,r.kt)("admonition",{type:"info"},(0,r.kt)("p",{parentName:"admonition"},"When a ",(0,r.kt)("inlineCode",{parentName:"p"},"StreamResolver")," is interpreted during a query or mutation operation, only the head element is respected.")),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"Technically, a ",(0,r.kt)("inlineCode",{parentName:"p"},"StreamResolver")," with one element can perform the same task as an ",(0,r.kt)("inlineCode",{parentName:"p"},"EffectResolver")," by creating a single-element stream.\nAn ",(0,r.kt)("inlineCode",{parentName:"p"},"EffectResolver")," has less resource overhead, since it is a single function compared to a ",(0,r.kt)("inlineCode",{parentName:"p"},"StreamResolver")," that has some bookkeeping associated with it.")),(0,r.kt)("p",null,"When the first element arrives, the interpreter will continue interpreting the rest of the query.\nThat is, the interpreter will be blocked until the first element arrives."),(0,r.kt)("admonition",{type:"caution"},(0,r.kt)("p",{parentName:"admonition"},"Streams may not be empty.\nIf stream terminates before at-least one element is emitted the subscription is terminated with an error.\nIf you for whatever reason whish to do this and permantently block a graphql subscription, you can always use ",(0,r.kt)("inlineCode",{parentName:"p"},"fs2.Stream.never"),".")),(0,r.kt)("admonition",{type:"danger"},(0,r.kt)("p",{parentName:"admonition"},"It is ",(0,r.kt)("strong",{parentName:"p"},"highly")," reccomended that every stream has at least one ",(0,r.kt)("strong",{parentName:"p"},"guarenteed")," element.\nThe initial evaluation of a (sub-)tree will never complete if a stream never emits, even if future updates cause a stream to become redundant.")),(0,r.kt)("p",null,"Whenever the tail of the stream emits, the interpreter will re-evaluate the sub-tree that occurs at the ",(0,r.kt)("inlineCode",{parentName:"p"},"StreamResolver"),".\nRe-evaluation forgets everything regarding the previous children, which includes ",(0,r.kt)("inlineCode",{parentName:"p"},"StreamResolver"),"s that may occur as children.\nForgotten ",(0,r.kt)("inlineCode",{parentName:"p"},"StreamResolver"),"s are gracefully cancelled."),(0,r.kt)("p",null,"Streaming is implemented in a sort of global re-evaluation loop.\nHaving global re-evaluation allows much more stable result emission and better batching possibilities."),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"If the reader is faimilar with the frontend framework ",(0,r.kt)("inlineCode",{parentName:"p"},"React"),", this works much in the same way.\n",(0,r.kt)("inlineCode",{parentName:"p"},"useEffect")," is analogous to resources in ",(0,r.kt)("inlineCode",{parentName:"p"},"fs2.Stream"),", updates are batched and the interpreter diffs previous and new trees and then effeciently applies whatever changes are necessary.")),(0,r.kt)("p",null,"An alternative formulation could involve letting every ",(0,r.kt)("inlineCode",{parentName:"p"},"StreamResolver")," have it's own re-evaluation loop, but this can have unforeseen consequences.\nFor instance, the implementation of nested streams becomes ambiguous.\nDoes an inner stream cancel when the outer emits? Does this mean that a fast outer stream can end up causing an inner stream to never emit?"),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},'The interpreter only respects elements arriving in streams that are considered "active".\nThat is, if a node emits but is also about to be removed because a parent has emitted, the interpreter will ignore the emission.')),(0,r.kt)("h3",{id:"interesting-use-cases"},"Interesting use cases"),(0,r.kt)("p",null,"Since stream can embed ",(0,r.kt)("inlineCode",{parentName:"p"},"Resource"),"s, some very interesting problems can be solved with ",(0,r.kt)("inlineCode",{parentName:"p"},"StreamResolver"),"s."),(0,r.kt)("p",null,"Say we had a very slow connection to some VPN server that we wanted to fetch data from, but only if data from the VPN had been selected."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import cats.effect.implicits._\nimport scala.concurrent.duration._\n\nfinal case class VpnData(\n  content: String,\n  hash: String,\n  connectedUser: String,\n  serverId: String\n)\n\ntype Username = String\n\ntrait VpnConnection[F[_]] {\n  def getName: F[String]\n  \n  def getCreatedAge: F[Int]\n  \n  def getDataUpdates: fs2.Stream[F, VpnData]\n}\n\nobject VpnConnection {\n  // Connection aquisition is very slow\n  def apply[F[_]](userId: Username, serverId: String)(implicit F: Async[F]): Resource[F, VpnConnection[F]] = {\n    Resource.eval(F.monotonic).flatMap{ before =>\n      Resource.makeFull[F, VpnConnection[F]]{ poll =>\n        poll{\n          F.delay(println(s"Connecting to VPN for $userId ...")) >>\n          F.sleep(500.millis) >> \n          F.delay(println(s"Connected to VPN for $userId!"))\n        }.onCancel(F.delay(println(s"Connection for $userId cancelled while connecting!"))).as{\n          new VpnConnection[F] {\n            def getName = F.delay("super_secret_file")\n            \n            def getCreatedAge = F.delay(42)\n            \n            def getDataUpdates = \n              fs2.Stream(1)\n                .repeat\n                .scan(0)(_ + _)\n                .lift[F]\n                .metered(50.millis)\n                .map{ x => println(s"emitting for user $userId");x}\n                .map(i => VpnData(s"content $i", s"hash of $i", userId, serverId))\n          }\n        }\n      }(_ => F.monotonic.map{ after => \n        println(s"Disconnecting from VPN after ${(after - before).toMillis}ms for $userId ...")\n      })\n    }\n  }\n}\n')),(0,r.kt)("p",null,"We could embed the VPN connection in a stream and pass it around to types that need it."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'final case class WithVpn[F[_], A](\n  vpn: VpnConnection[F],\n  value: A\n)\n\nfinal case class VpnMetadata(subscriptionTimestamp: String)\n\ndef currentTimestamp[F[_]](implicit F: Applicative[F]): F[String] = F.pure("now!")\n\nimplicit def vpnMetadata[F[_]: Applicative] = tpe[F, WithVpn[F, VpnMetadata]](\n  "VpnMetadata",\n  "name" -> eff(_.vpn.getName),\n  "createdAge" -> eff(_.vpn.getCreatedAge),\n  "subscriptionTimestamp" -> pure(_.value.subscriptionTimestamp),\n)\n\nimplicit def vpnData[F[_]: Applicative] = tpe[F, VpnData](\n  "VpnData",\n  "content" -> pure(_.content),\n  "hash" -> pure(_.hash),\n  "connectedUser" -> pure(_.connectedUser),\n  "serverId" -> pure(_.serverId)\n)\n\nimplicit def vpn[F[_]: Applicative] = tpe[F, VpnConnection[F]](\n  "Vpn",\n  "metadata" -> eff(conn => currentTimestamp[F].map(ts => WithVpn(conn, VpnMetadata(ts)))),\n  "data" -> field(stream(_.getDataUpdates))\n)\n\ndef root[F[_]: Async] = \n  tpe[F, Username](\n    "Subscription",\n    "vpn" -> field(arg[String]("serverId"))(stream{ case (userId, serverId) => \n      fs2.Stream.resource(VpnConnection[F](userId, serverId))\n    }),\n    "me" -> pure(identity)\n  )\n')),(0,r.kt)("p",null,"We can now try querying the VPN connection through a GraphQL query:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import gql.ast._\n\ndef subscriptionQuery = """\nsubscription {\n  vpn(serverId: "secret_server") {\n    metadata {\n      name\n      createdAge\n      subscriptionTimestamp\n    }\n    data {\n      content\n      hash\n      connectedUser\n      serverId\n    }\n  }\n}\n"""\n\ndef runVPNSubscription(q: String, n: Int, subscription: Type[IO, Username] = root[IO]) = \n  Schema.simple(SchemaShape[IO, Unit, Unit, Username](subscription = subscription.some)).flatMap{ sch =>\n    sch.assemble(q, variables = Map.empty)\n      .traverse { case Executable.Subscription(run) => \n        run("john_doe").take(n).map(_.asGraphQL).compile.toList \n      }\n  }\n  \nrunVPNSubscription(subscriptionQuery, 3).unsafeRunSync()\n// Connecting to VPN for john_doe ...\n// Connected to VPN for john_doe!\n// emitting for user john_doe\n// emitting for user john_doe\n// emitting for user john_doe\n// Disconnecting from VPN after 673ms for john_doe ...\n// res3: Either[parser.package.ParseError, List[io.circe.JsonObject]] = Right(\n//   value = List(\n//     object[data -> {\n//   "vpn" : {\n//     "data" : {\n//       "serverId" : "secret_server",\n//       "connectedUser" : "john_doe",\n//       "hash" : "hash of 0",\n//       "content" : "content 0"\n//     },\n//     "metadata" : {\n//       "subscriptionTimestamp" : "now!",\n//       "createdAge" : 42,\n//       "name" : "super_secret_file"\n//     }\n//   }\n// }],\n//     object[data -> {\n//   "vpn" : {\n//     "data" : {\n//       "serverId" : "secret_server",\n//       "connectedUser" : "john_doe",\n//       "hash" : "hash of 1",\n//       "content" : "content 1"\n//     },\n//     "metadata" : {\n//       "subscriptionTimestamp" : "now!",\n//       "createdAge" : 42,\n//       "name" : "super_secret_file"\n//     }\n//   }\n// }],\n//     object[data -> {\n//   "vpn" : {\n//     "data" : {\n//       "serverId" : "secret_server",\n//       "connectedUser" : "john_doe",\n//       "hash" : "hash of 2",\n//       "content" : "content 2"\n//     },\n//     "metadata" : {\n//       "subscriptionTimestamp" : "now!",\n//       "createdAge" : 42,\n//       "name" : "super_secret_file"\n//     }\n//   }\n// }]\n//   )\n// )\n')),(0,r.kt)("p",null,"We can alsa check the performance difference of a queries that open a VPN connection, and ones that don't:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'def bench(fa: IO[_]) = \n  for {\n    before <- IO.monotonic\n    _ <- fa.timed\n    after <- IO.monotonic\n  } yield s"duration was ${(after - before).toMillis}ms"\n  \nbench(runVPNSubscription(subscriptionQuery, 10)).unsafeRunSync()\n// Connecting to VPN for john_doe ...\n// Connected to VPN for john_doe!\n// emitting for user john_doe\n// emitting for user john_doe\n// emitting for user john_doe\n// emitting for user john_doe\n// emitting for user john_doe\n// emitting for user john_doe\n// emitting for user john_doe\n// emitting for user john_doe\n// emitting for user john_doe\n// emitting for user john_doe\n// Disconnecting from VPN after 1008ms for john_doe ...\n// res4: String = "duration was 1020ms"\n\nbench(runVPNSubscription(subscriptionQuery, 3)).unsafeRunSync()\n// Connecting to VPN for john_doe ...\n// Connected to VPN for john_doe!\n// emitting for user john_doe\n// emitting for user john_doe\n// emitting for user john_doe\n// Disconnecting from VPN after 653ms for john_doe ...\n// res5: String = "duration was 659ms"\n\nbench(runVPNSubscription(subscriptionQuery, 1)).unsafeRunSync()\n// Connecting to VPN for john_doe ...\n// Connected to VPN for john_doe!\n// emitting for user john_doe\n// Disconnecting from VPN after 562ms for john_doe ...\n// res6: String = "duration was 569ms"\n\ndef fastQuery = """\n  subscription {\n    me\n  }\n"""\n\nbench(runVPNSubscription(fastQuery, 1)).unsafeRunSync()\n// res7: String = "duration was 5ms"\n')),(0,r.kt)("p",null,"Say that the VPN connection was based on a OAuth token that needed to be refreshed every 600 milliseconds.\nThis is also possible:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'def oauthAccessToken[F[_]: Async](username: Username): fs2.Stream[F, Username] =\n  fs2.Stream(username)\n    .lift[F]\n    .repeat\n    .metered(600.millis)\n    .zipWithIndex\n    .map{ case (un, i) => s"token-$un-$i"}\n    .map{x => println(s"a new token was issued: $x");x}\n\ndef root2[F[_]: Async] = \n  tpe[F, Username](\n    "Subscription",\n    "vpn" -> field(arg[String]("serverId"))(stream{ case (userId, serverId) => \n      oauthAccessToken[F](userId).flatMap{ token =>\n        fs2.Stream.resource(VpnConnection[F](token, serverId))\n      }\n    })\n  )\n  \nrunVPNSubscription(subscriptionQuery, 13, root2[IO]).unsafeRunSync().map(_.takeRight(3))\n// a new token was issued: token-john_doe-0\n// Connecting to VPN for token-john_doe-0 ...\n// Connected to VPN for token-john_doe-0!\n// emitting for user token-john_doe-0\n// a new token was issued: token-john_doe-1\n// Connecting to VPN for token-john_doe-1 ...\n// emitting for user token-john_doe-0\n// emitting for user token-john_doe-0\n// emitting for user token-john_doe-0\n// emitting for user token-john_doe-0\n// emitting for user token-john_doe-0\n// emitting for user token-john_doe-0\n// emitting for user token-john_doe-0\n// emitting for user token-john_doe-0\n// emitting for user token-john_doe-0\n// emitting for user token-john_doe-0\n// Connected to VPN for token-john_doe-1!\n// emitting for user token-john_doe-0\n// emitting for user token-john_doe-0\n// emitting for user token-john_doe-1\n// Disconnecting from VPN after 1168ms for token-john_doe-0 ...\n// a new token was issued: token-john_doe-2\n// Connecting to VPN for token-john_doe-2 ...\n// emitting for user token-john_doe-1\n// Connection for token-john_doe-2 cancelled while connecting!\n// Disconnecting from VPN after 619ms for token-john_doe-1 ...\n// res8: Either[parser.package.ParseError, List[io.circe.JsonObject]] = Right(\n//   value = List(\n//     object[data -> {\n//   "vpn" : {\n//     "data" : {\n//       "serverId" : "secret_server",\n//       "connectedUser" : "token-john_doe-0",\n//       "hash" : "hash of 10",\n//       "content" : "content 10"\n//     },\n//     "metadata" : {\n//       "subscriptionTimestamp" : "now!",\n//       "createdAge" : 42,\n//       "name" : "super_secret_file"\n//     }\n//   }\n// }],\n//     object[data -> {\n//   "vpn" : {\n//     "data" : {\n//       "serverId" : "secret_server",\n//       "connectedUser" : "token-john_doe-1",\n//       "hash" : "hash of 0",\n//       "content" : "content 0"\n//     },\n//     "metadata" : {\n//       "subscriptionTimestamp" : "now!",\n//       "createdAge" : 42,\n//       "name" : "super_secret_file"\n//     }\n//   }\n// }],\n//     object[data -> {\n//   "vpn" : {\n//     "data" : {\n//       "serverId" : "secret_server",\n//       "connectedUser" : "token-john_doe-1",\n//       "hash" : "hash of 1",\n//       "content" : "content 1"\n//     },\n//     "metadata" : {\n//       "subscriptionTimestamp" : "now!",\n//       "createdAge" : 42,\n//       "name" : "super_secret_file"\n//     }\n//   }\n// }]\n//   )\n// )\n')))}m.isMDXComponent=!0}}]);